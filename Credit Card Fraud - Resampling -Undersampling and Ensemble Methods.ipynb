{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score,roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas_ml import ConfusionMatrix\n",
    "creditcard = pd.read_csv('xxxxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reading the first few lines of data\n",
    "creditcard.head(10)\n",
    "#Column time denotes time from the first transaction. V1 to V28 represents confidential information which seems to have \n",
    "#been normalized. \n",
    "#column amount represents the transaction money for that specific time \n",
    "#class represents whether the transaction is fraud (label:1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting a sense of the data \n",
    "creditcard.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "creditcard.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking for missing cell/data\n",
    "creditcard.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking the number of unique values in class column \n",
    "creditcard['Class'].unique() #there are 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting the number of 0s and 1s in that column \n",
    "pd.value_counts(creditcard['Class'].values, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The percentage of fraudulent transactions are:{:.3f}'.format((creditcard['Class']==1).sum()/len(creditcard['Class'])*100),'%')\n",
    "print('The percentage of non fraudulent transactions are:{:.3f}'.format((creditcard['Class']==0).sum()/len(creditcard['Class'])*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#As can be seen, this is a case of highly imbalanced data. There are several approaches to dealing with this data and \n",
    "#build a model that will be able to detect these fraudulent transactions which right now will look as outliers.\n",
    "#The first approach is to do nothing and adjust the class_weights = balanced and look at the recall, F1_scores as well as the confusion matrix\n",
    "#Accuracy scores do not work well when working with imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Prior to running the models, time and amount columns will be standardized since columns from V1 to V28 have undergone PCA \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(np.array(creditcard['Amount']).reshape(-1,1))\n",
    "scaled_amount = scaler.transform(np.array(creditcard['Amount']).reshape(-1,1))\n",
    "scaler1 = StandardScaler().fit(np.array(creditcard['Time']).reshape(-1,1))\n",
    "scaled_time = scaler1.transform(np.array(creditcard['Time']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating a 2nd dataframe with scaled amount and scaled time as columns. Will drop the original \"time\" and \"amount\" column\n",
    "df = pd.DataFrame(scaled_amount,columns =['scaled_amount'])\n",
    "df2 = pd.DataFrame(scaled_time,columns =['scaled_time'])\n",
    "credit_card_ss = pd.concat([creditcard,df, df2], axis=1)\n",
    "cols_to_drop =['Time','Amount']\n",
    "credit_card_ss = credit_card_ss[credit_card_ss.columns.drop(cols_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "credit_card_ss.head(5) #Checking that standardizing of specified columns have occurred, appended to a new dataset and dropping of \"time\" \n",
    "#amount columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_credit_card_ss = credit_card_ss.drop('Class', axis=1)\n",
    "y_credit_card_ss = credit_card_ss['Class']\n",
    "X_credit_card_ss_train, X_credit_card_ss_test, y_credit_card_ss_train, y_credit_card_ss_test = train_test_split(X_credit_card_ss, y_credit_card_ss, test_size=0.3, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now we can use some classifiers on this dataset and check the scores. will adopt the balanced approach for the class weights to deal with the imbalanced dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "lr1 = LogisticRegression(class_weight='balanced', random_state=40)\n",
    "clf1 = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=60, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight='balanced', presort=False)\n",
    "svm1 = svm.SVC(kernel='rbf', C = 1.0, class_weight ='balanced', random_state=40)\n",
    "#Training models\n",
    "model_lr1 = lr1.fit(X_credit_card_ss_train,y_credit_card_ss_train)\n",
    "model_clf1 = clf1.fit(X_credit_card_ss_train,y_credit_card_ss_train)\n",
    "model_svm1 = svm1.fit(X_credit_card_ss_train,y_credit_card_ss_train)\n",
    "#Prediction on test set \n",
    "pred_y_lr1 = lr1.predict(X_credit_card_ss_test)\n",
    "pred_y_clf1 = clf1.predict(X_credit_card_ss_test)\n",
    "pred_y_svm1 = svm1.predict(X_credit_card_ss_test)\n",
    "#Printing recall and F1_scores \n",
    "print ('Recall score of logistic regression classifier on test set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_lr1)))\n",
    "print ('F1 score of logistic regression classifier on test set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_lr1)))\n",
    "cm_lr1 = confusion_matrix(y_credit_card_ss_test,pred_y_lr1)\n",
    "print('Confusion matrix with logistic regression classifier with on test set:\\n%s' % cm_lr1)\n",
    "print('\\n')\n",
    "print ('Recall score of decision tree classifier on test set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_clf1)))\n",
    "print ('F1 score of decision tree classifier on test set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_clf1)))\n",
    "cm_clf1 = confusion_matrix(y_credit_card_ss_test,pred_y_clf1)\n",
    "print('Confusion matrix with decision tree classifier with on test set:\\n%s' % cm_clf1)\n",
    "print('\\n')\n",
    "print ('Recall score of SVM classifier on test set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_svm1)))\n",
    "print ('F1 score of SVM classifier on test set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_svm1)))\n",
    "cm_svm1 = confusion_matrix(y_credit_card_ss_test,pred_y_svm1)\n",
    "print('Confusion matrix with decision tree classifier with on test set:\\n%s' % cm_svm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#As can be seen, both models have not done so well in predicting the fraud cases. In both models, they were mostly classified as \n",
    "# false negative ie non fraud cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#USe cross validation score cross_val_score to redo the models and check for any improvements in recall and f1 scores\n",
    "# Perform 6-fold cross validation\n",
    "recall_lr1 = cross_val_score(lr1, X_credit_card_ss , y_credit_card_ss, scoring='recall', cv = 6)\n",
    "print(\"Recall score for logistic regression with balanced class weight: %0.3f (+/- %0.3f)\" % (recall_lr1.mean(), recall_lr1.std() * 2))\n",
    "f1_lr1 = cross_val_score(lr1,X_credit_card_ss , y_credit_card_ss, scoring='f1', cv = 6)\n",
    "print(\"F1 score for logistic regression with balanced class weight: %0.3f (+/- %0.3f)\" % (f1_lr1.mean(), f1_lr1.std() * 2))\n",
    "recall_clf1 = cross_val_score(clf1,X_credit_card_ss , y_credit_card_ss, scoring='recall', cv = 6)\n",
    "print(\"Recall score for decision tree classifier with balanced class weight: %0.3f (+/- %0.3f)\" % (recall_clf1.mean(), recall_clf1.std() * 2))\n",
    "f1_clf1 = cross_val_score(clf1, X_credit_card_ss , y_credit_card_ss, scoring='f1', cv = 6)\n",
    "print(\"F1 score for decision tree classifier with balanced class weight: %0.3f (+/- %0.3f)\" % (f1_clf1.mean(), f1_clf1.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting an idea of transaction amount over time\n",
    "import matplotlib.pyplot as plt\n",
    "x = creditcard['Time']\n",
    "y = creditcard['Amount']\n",
    "plt.title('Trend in transaction amount')\n",
    "plt.xlabel('Time,s')\n",
    "plt.ylabel('Amount')\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Is there some correlation between the variables in the dataset \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize = (20,10))\n",
    "sns.heatmap(credit_card_ss.corr(), annot=True, fmt=\".2f\")\n",
    "plt.show()\n",
    "#Based on the heat map, there are some negative correlations observed in the dataset for example: \n",
    "# time and v3, v2 and amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Two approaches to make a balanced dataset out of an imbalanced one are under-sampling and over-sampling.\n",
    "\n",
    "#Under-sampling\n",
    "#Under-sampling balances the dataset by reducing the size of the abundant class. This method is used when quantity of data is \n",
    "#sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, \n",
    "#a balanced new dataset can be retrieved for further modelling.Generating centroid based on a clustering method (e.g. K-Means) \n",
    "#is a common strategy for this\n",
    "\n",
    "#Oversampling\n",
    "#On the contrary, oversampling is used when the quantity of data is insufficient. It tries to balance dataset by increasing \n",
    "#the size of rare samples. Rather than getting rid of abundant samples, new rare samples are generated by \n",
    "#using e.g. repetition, bootstrapping or SMOTE (Synthetic Minority Over-Sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using under sampling approach to create a balanced dataset\n",
    "#Undersample the class = 0 which represents non fraudulent transactions\n",
    "from sklearn.utils import resample\n",
    "creditcard_nonfraud = credit_card_ss[credit_card_ss.Class==0]\n",
    "creditcard_fraud = credit_card_ss[credit_card_ss.Class==1]\n",
    "#Taking a percentage of fraudulent cases ~61%. Will select 300 cases of fraudulent activities without replacement and leave \n",
    "#192 cases in original dataset to models predict their class. 192 cases will be part of the unseen data.\n",
    "creditcard_undersampled = resample(credit_card_ss[credit_card_ss.Class==1], \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=300,     # represent around 61% of fraudulent cases\n",
    "                                 random_state=75) # reproducible results\n",
    "\n",
    "# Undersample majority class\n",
    "creditcard_nonfraud_undersampled = resample(credit_card_ss[credit_card_ss.Class==0], \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=300,     # to match minority class\n",
    "                                 random_state=70) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "creditcard_undersampled = pd.concat([creditcard_nonfraud_undersampled, creditcard_undersampled])\n",
    " \n",
    "# Display new class counts\n",
    "print(creditcard_undersampled.Class.value_counts())\n",
    "\n",
    "\n",
    "#Creating new dataframe which does not contain the data that is present in creditcard_undersampled dataset\n",
    "creditcard_original_leftover = credit_card_ss.loc[~credit_card_ss.set_index(list(credit_card_ss.columns)).index.isin(creditcard_undersampled.set_index(list(creditcard_undersampled.columns)).index)]\n",
    "#Checking shape of all datasets\n",
    "print ('Dimension of creditcard_original_leftover:'+str(creditcard_original_leftover.shape))\n",
    "print('Dimension of creditcard_undersampled:',str(creditcard_undersampled.shape))\n",
    "print('Dimension of creditcard:',str(credit_card_ss.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We now have an undersample which consists 50% of fraudulent and non fraudulent transactions. \n",
    "#We will train both supervised and unsupervised learning classifers on this undersampled dataset, then compare their performances\n",
    "#on the original dataset.\n",
    "#Some of the classifiers from the supervised learning families that will be used are:\n",
    "#1.logistic regression((penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, \n",
    "#solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None))\n",
    "#2.decision tree (criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None,\n",
    "#random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n",
    "#3.SVC ((C=1.0, kernel=’rbf’, degree=3, gamma=’auto_deprecated’, coef0=0.0, shrinking=True, probability=False, tol=0.001, \n",
    "#cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Splitting undersampled dataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_under = creditcard_undersampled.drop('Class', axis=1)\n",
    "y_under = creditcard_undersampled['Class']\n",
    "X_under_train, X_under_test, y_under_train, y_under_test = train_test_split(X_under, y_under, test_size=0.3, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use logistic regression on undersampeld dataset and get the scores. \n",
    "#Will use gridsearch as well to see which hyperparameters give the best recall and f1 scores. \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameter_candidates = [\n",
    "  {'penalty':['l2'],'C': [0.001,0.01,0.1,1, 10, 100, 1000,10000]},\n",
    "  {'penalty':['l1'],'C': [0.001,0.01,0.1,1, 10, 100, 1000,10000]},\n",
    "]\n",
    "cv_range=[3,5,10,15,20]\n",
    "for i in cv_range:\n",
    "    lr_under = GridSearchCV(estimator=LogisticRegression(random_state=90), param_grid=parameter_candidates, n_jobs=-1, cv=i)\n",
    "\n",
    "#Training set \n",
    "    lr_under.fit(X_under_train,y_under_train)\n",
    "\n",
    "#Prediction on training set - undersampled \n",
    "    pred_y_lr_under = lr_under.predict(X_under_test)\n",
    "\n",
    "#Getting accuracy scores on the undersampled dataset \n",
    "#Accuracy and Recall scores. \n",
    "print ('Accuracy score of logistic regression classifier on test set:{:.3f}'.format(accuracy_score(y_under_test,pred_y_lr_under)))\n",
    "print ('Recall score of logistic regression classifier on test set:{:.3f}'.format(recall_score(y_under_test,pred_y_lr_under)))\n",
    "print ('F1 score of logistic regression classifier on test set:{:.3f}'.format(f1_score(y_under_test,pred_y_lr_under)))\n",
    "print()\n",
    "cm_lr_under = confusion_matrix(y_under_test,pred_y_lr_under)\n",
    "print('Confusion matrix with logistic regression classifier with on undersampled test set:\\n%s' % cm_lr_under)\n",
    "print()\n",
    "print('Best C for logistic regression:',lr_under.best_estimator_.C) \n",
    "print('Best penalty:',lr_under.best_estimator_.penalty)\n",
    "print('The best paramaters for the logistic regression classifier according to GridSearch and CV = %r:'% (i),lr_under.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test on original dataset without the dataset used for the undersampled set \n",
    "#Prediction on original creditcard dataset \n",
    "X_original = creditcard_original_leftover.drop('Class', axis=1)\n",
    "y_original = creditcard_original_leftover['Class']\n",
    "\n",
    "pred_y_original_lr_under = lr_under.predict(X_original)\n",
    "\n",
    "#Since this original dataset is an imbalanced dataset, the accuracy scores will be biased towards the majority class. Will use \n",
    "#recall, f1 scores and confusion matrix\n",
    "print ('Recall score of logistic regression classifier on original set:{:.3f}'.format(recall_score(y_original,pred_y_original_lr_under)))\n",
    "print ('F1 score of logistic regression classifier on original set:{:.3f}'.format(f1_score(y_original,pred_y_original_lr_under)))\n",
    "print()\n",
    "cm_lr_under_original = confusion_matrix(y_original,pred_y_original_lr_under)\n",
    "print('Confusion matrix with logistic regression classifier with on original set:\\n%s' % cm_lr_under_original)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#using decision tree classifier on undersampled training set\n",
    "#Will use gridsearch as well to see which hyperparameters give the best recall and f1 scores. \n",
    "\n",
    "parameter_candidates = {'max_features': ['log2', 'sqrt','auto'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2, 3, 5, 10], \n",
    "              'min_samples_split': [2, 3, 5],\n",
    "              'min_samples_leaf': [1,5,8]\n",
    "             }\n",
    "cv_range=[3,5,10,15,20]\n",
    "for i in cv_range:\n",
    "    dt_under = GridSearchCV(estimator=DecisionTreeClassifier(random_state=75), param_grid=parameter_candidates, n_jobs=-1, cv=i)\n",
    "\n",
    "#Training set \n",
    "    dt_under.fit(X_under_train,y_under_train)\n",
    "\n",
    "#Prediction on training set - undersampled \n",
    "    pred_y_dt_under = dt_under.predict(X_under_test)\n",
    "\n",
    "#Getting accuracy scores on the undersampled dataset \n",
    "#Accuracy and Recall scores. \n",
    "print ('Accuracy score of decision tree classifier on test set:{:.3f}'.format(accuracy_score(y_under_test,pred_y_dt_under)))\n",
    "print ('Recall score of decision tree classifier on test set:{:.3f}'.format(recall_score(y_under_test,pred_y_dt_under)))\n",
    "print ('F1 score of decision tree classifier on test set:{:.3f}'.format(f1_score(y_under_test,pred_y_dt_under)))\n",
    "print()\n",
    "cm_dt_under = confusion_matrix(y_under_test,pred_y_dt_under)\n",
    "print('Confusion matrix with decision tree classifier with on undersampled test set:\\n%s' % cm_dt_under)\n",
    "print()\n",
    "print('The best paramaters for the decision tree classifier according to GridSearch and CV = %r:'% (i),dt_under.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing decision tree classifier on original dataset \n",
    "pred_y_original_dt_under = dt_under.predict(X_original)\n",
    "\n",
    "#Since this original dataset is an imbalanced dataset, the accuracy scores will be biased towards the majority class. Will use \n",
    "#recall, f1 scores and confusion matrix\n",
    "print ('Recall score of decision tree classifier on original set:{:.3f}'.format(recall_score(y_original,pred_y_original_dt_under)))\n",
    "print ('F1 score of decision tree classifier on original set:{:.3f}'.format(f1_score(y_original,pred_y_original_dt_under)))\n",
    "print()\n",
    "cm_dt_under_original = confusion_matrix(y_original,pred_y_original_dt_under)\n",
    "print('Confusion matrix with decision tree classifier on original set:\\n%s' % cm_dt_under_original)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using SVM classifier \n",
    "#Will use gridsearch as well to see which hyperparameters give the best recall and f1 scores. \n",
    "from sklearn import svm\n",
    "parameter_candidates = {'C': [0.001, 0.01, 1, 10, 100, 1000], \n",
    "              'kernel': ['rbf', 'linear','poly'],\n",
    "              'gamma': [0.001, 0.01, 1, 10, 100, 1000] \n",
    "             }\n",
    "cv_range=[3,5,10,15,20]\n",
    "for i in cv_range:\n",
    "    svc_under = GridSearchCV(estimator=svm.SVC(random_state=60), param_grid=parameter_candidates, n_jobs=-1, cv=i)\n",
    "\n",
    "#Training set \n",
    "    svc_under.fit(X_under_train,y_under_train)\n",
    "\n",
    "#Prediction on training set - undersampled \n",
    "    pred_y_svc_under = svc_under.predict(X_under_test)\n",
    "\n",
    "#Getting accuracy scores on the undersampled dataset \n",
    "#Accuracy and Recall scores. \n",
    "print ('Accuracy score of SVM classifier on test set:{:.3f}'.format(accuracy_score(y_under_test,pred_y_svc_under)))\n",
    "print ('Recall score of SVM classifier on test set:{:.3f}'.format(recall_score(y_under_test,pred_y_svc_under)))\n",
    "print ('F1 score of SVM classifier on test set:{:.3f}'.format(f1_score(y_under_test,pred_y_svc_under)))\n",
    "print()\n",
    "cm_svc_under = confusion_matrix(y_under_test,pred_y_svc_under)\n",
    "print('Confusion matrix with SVM classifier with on undersampled test set:\\n%s' % cm_svc_under)\n",
    "print()\n",
    "print('The best paramaters for the SVM classifier according to GridSearch and CV = %r:'% (i),svc_under.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing decision tree classifier on original dataset \n",
    "pred_y_original_svc_under = svc_under.predict(X_original)\n",
    "\n",
    "#Since this original dataset is an imbalanced dataset, the accuracy scores will be biased towards the majority class. Will use \n",
    "#recall, f1 scores and confusion matrix\n",
    "print ('Recall score of SVM classifier on original set:{:.3f}'.format(recall_score(y_original,pred_y_original_svc_under)))\n",
    "print ('F1 score of SVM classifier on original set:{:.3f}'.format(f1_score(y_original,pred_y_original_svc_under)))\n",
    "print()\n",
    "cm_svc_under_original = confusion_matrix(y_original,pred_y_original_svc_under)\n",
    "print('Confusion matrix with SVM classifier on original set:\\n%s' % cm_svc_under_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use ensemble algorithm without hyperparameter tuning to see if there is any improvement in F1 score. Will use adaboost \n",
    "#in combination with decision tree classifier. \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=200,random_state = 55)\n",
    "\n",
    "#Training set \n",
    "bdt.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_bdt=bdt.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of AdaBoosted tree decision classifier on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_bdt)))\n",
    "print ('F1 score of AdaBoosted tree decision classifier on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_bdt)))\n",
    "print()\n",
    "cm_bdt_original = confusion_matrix(y_credit_card_ss_test,pred_y_bdt)\n",
    "print('Confusion matrix with AdaBoosted tree decision classifier on original set:\\n%s' % cm_bdt_original)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use ensemble algorithm without hyperparameter tuning to see if there is any improvement in F1 score. Will use adaboost \n",
    "#in combination with decision tree classifier. \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=300,random_state = 55)\n",
    "\n",
    "#Training set \n",
    "bdt.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_bdt=bdt.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of AdaBoosted tree decision classifier on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_bdt)))\n",
    "print ('F1 score of AdaBoosted tree decision classifier on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_bdt)))\n",
    "print()\n",
    "cm_bdt_original = confusion_matrix(y_credit_card_ss_test,pred_y_bdt)\n",
    "print('Confusion matrix with AdaBoosted tree decision classifier on original set:\\n%s' % cm_bdt_original)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use Adaboost classifier by itself and see the performance \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=90)\n",
    "\n",
    "#Training set \n",
    "ada.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_ada=ada.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of AdaBoost classifier on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_ada)))\n",
    "print ('F1 score of AdaBoost classifier on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_ada)))\n",
    "print()\n",
    "cm_ada_original = confusion_matrix(y_credit_card_ss_test,pred_y_ada)\n",
    "print('Confusion matrix with AdaBoost Classifier on original set:\\n%s' % cm_ada_original)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use Adaboost classifier by itself and see the performance \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(n_estimators=200, random_state=90)\n",
    "\n",
    "#Training set \n",
    "ada.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_ada=ada.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of AdaBoost classifier on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_ada)))\n",
    "print ('F1 score of AdaBoost classifier on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_ada)))\n",
    "print()\n",
    "cm_ada_original = confusion_matrix(y_credit_card_ss_test,pred_y_ada)\n",
    "print('Confusion matrix with AdaBoost Classifier on original set:\\n%s' % cm_ada_original)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use GradientBoosting classifier by itself and see the performance \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbm = GradientBoostingClassifier(n_estimators=200, random_state=90)\n",
    "\n",
    "#Training set \n",
    "gbm.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_gbm=gbm.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of GradientBoost classifier on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_gbm)))\n",
    "print ('F1 score of GradientBoost classifier on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_gbm)))\n",
    "print()\n",
    "cm_gbm_original = confusion_matrix(y_credit_card_ss_test,pred_y_gbm)\n",
    "print('Confusion matrix with GradientBoost Classifier on original set:\\n%s' % cm_gbm_original)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use GradientBoosting classifier by itself and see the performance \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbm = GradientBoostingClassifier(n_estimators=300, random_state=90)\n",
    "\n",
    "#Training set \n",
    "gbm.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_gbm=gbm.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of GradientBoost classifier on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_gbm)))\n",
    "print ('F1 score of GradientBoost classifier on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_gbm)))\n",
    "print()\n",
    "cm_gbm_original = confusion_matrix(y_credit_card_ss_test,pred_y_gbm)\n",
    "print('Confusion matrix with GradientBoost Classifier on original set:\\n%s' % cm_gbm_original)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use extreme gradient boosting to see if there is any improvement in scores in comparison to gradient boosting -no hyperparameter tuning \n",
    "#Use GradientBoosting classifier by itself and see the performance \n",
    "import xgboost as xgb\n",
    "xgb = xgb.XGBClassifier(n_estimators=300 ,random_state = 105)\n",
    "\n",
    "#Training set \n",
    "xgb.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_xgb=xgb.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of Extreme GradientBoost classifier on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_xgb)))\n",
    "print ('F1 score of Extreme GradientBoost GradientBoost classifier on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_xgb)))\n",
    "print()\n",
    "cm_xgb_original = confusion_matrix(y_credit_card_ss_test,pred_y_xgb)\n",
    "print('Confusion matrix with Extreme GradientBoost Classifier on original set:\\n%s' % cm_xgb_original)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use extreme gradient boosting to see if there is any improvement in scores in comparison to gradient boosting -no hyperparameter tuning \n",
    "#Use GradientBoosting classifier by itself and see the performance \n",
    "import xgboost as xgb\n",
    "xgb = xgb.XGBClassifier(n_estimators=200 ,random_state = 105)\n",
    "\n",
    "#Training set \n",
    "xgb.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_xgb=xgb.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of Extreme GradientBoost classifier on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_xgb)))\n",
    "print ('F1 score of Extreme GradientBoost GradientBoost classifier on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_xgb)))\n",
    "print()\n",
    "cm_xgb_original = confusion_matrix(y_credit_card_ss_test,pred_y_xgb)\n",
    "print('Confusion matrix with Extreme GradientBoost Classifier on original set:\\n%s' % cm_xgb_original)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use random forest on dataset to get recall and F1 scores \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "rf = RandomForestClassifier(n_estimators=100 ,criterion = 'gini', random_state = 30)\n",
    "\n",
    "#Training set \n",
    "rf.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_rf=rf.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of Random Forest classifier on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_rf)))\n",
    "print ('F1 score of Random Forest classifier on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_rf)))\n",
    "print()\n",
    "cm_rf_original = confusion_matrix(y_credit_card_ss_test,pred_y_rf)\n",
    "print('Confusion matrix with RandomForestClassifier Classifier on original set:\\n%s' % cm_rf_original)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use random forest on dataset to get recall and F1 scores \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "rf = RandomForestClassifier(n_estimators=200 ,criterion = 'gini', random_state = 30)\n",
    "\n",
    "#Training set \n",
    "rf.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_rf=rf.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of Random Forest classifier on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_rf)))\n",
    "print ('F1 score of Random Forest classifier on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_rf)))\n",
    "print()\n",
    "cm_rf_original = confusion_matrix(y_credit_card_ss_test,pred_y_rf)\n",
    "print('Confusion matrix with RandomForestClassifier Classifier on original set:\\n%s' % cm_rf_original)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use ensemble algorithms to see how they perform on original dataset. Supposedly, they can handle imbalanced dataset\n",
    "#Adaboost - works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm \n",
    "#to pay or or less attention to them in the construction of subsequent models.\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameter_candidates = {'n_estimators': [50, 200],  \n",
    "              'learning_rate': [0.01, 100]\n",
    "             }\n",
    " \n",
    "cv_range=[10,20]\n",
    "for i in cv_range:\n",
    "    ada_gs = GridSearchCV(estimator=AdaBoostClassifier(random_state = 50), param_grid=parameter_candidates, n_jobs=-1, cv=i)\n",
    "    \n",
    "#Training set \n",
    "ada_gs.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_ada_gs=ada_gs.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of AdaBoost classifier with gridsearch on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_ada_gs)))\n",
    "print ('F1 score of AdaBoost classifier with gridsearch on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_ada_gs)))\n",
    "print(\\n)\n",
    "cm_ada_gs_original = confusion_matrix(y_credit_card_ss_test,pred_y_ada_gs)\n",
    "print('Confusion matrix with AdaBoost classifier with gridsearch on original set:\\n%s' % cm_ada_gs_original)\n",
    "print(\\n)\n",
    "print('The best paramaters for the AdaBoost classifier according to GridSearch and CV = %r:'% (i),ada_gs.best_params_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use GradientBoosting classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "parameter_candidates = {'n_estimators': [50,200], \n",
    "              'learning_rate': [0.01, 1, 10, 100]\n",
    "             }\n",
    " \n",
    "cv_range=[10,20]\n",
    "for i in cv_range:\n",
    "    gbm = GridSearchCV(estimator=GradientBoostingClassifier(random_state = 55), param_grid=parameter_candidates, n_jobs=-1, cv=i)\n",
    "    \n",
    "#Training set \n",
    "gbm.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_gbm=gbm.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of GradientBoost classifier on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_gbm)))\n",
    "print ('F1 score of GradientBoost classifier on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_gbm)))\n",
    "print()\n",
    "cm_gbm_original = confusion_matrix(y_credit_card_ss_test,pred_y_gbm)\n",
    "print('Confusion matrix with GradientBoost classifier on original set:\\n%s' % cm_gbm_original)\n",
    "print()\n",
    "#print (ConfusionMatrix(y_credit_card_ss_test,pred_y_gbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use XGB (extreme gradient boosting)\n",
    "import xgboost as xgb\n",
    "parameter_candidates = {'n_estimators': [50, 200], \n",
    "              'learning_rate': [ 0.01, 100]\n",
    "             }\n",
    "cv_range=[10,20]\n",
    "for i in cv_range:\n",
    "    xgb = GridSearchCV(estimator=xgb.XGBClassifier(random_state = 105), param_grid=parameter_candidates, n_jobs=-1, cv=i)\n",
    "    \n",
    "#Training set \n",
    "xgb.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_xgb=xgb.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of XGBoost classifier on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_xgb)))\n",
    "print ('F1 score of XGBBoost classifier on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_xgb)))\n",
    "print()\n",
    "cm_xgb_original = confusion_matrix(y_credit_card_ss_test,pred_y_xgb)\n",
    "print('Confusion matrix with XGBBoost classifier on original set:\\n%s' % cm_xgb_original)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using random forest as another technique \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "parameter_candidates = {'n_estimators': [50,200], \n",
    "              'criterion': ['gini','entropy'],\n",
    "              'max_features': ['auto','sqrt','log2'],\n",
    "              'class_weight': ['none','balanced']        \n",
    "             }\n",
    "cv_range=[10,20]\n",
    "for i in cv_range:\n",
    "    rf = GridSearchCV(estimator=RandomForestClassifier(random_state = 105), param_grid=parameter_candidates, n_jobs=-1, cv=i)\n",
    "    \n",
    "#Training set \n",
    "rf.fit(X_credit_card_ss_train, y_credit_card_ss_train)\n",
    "\n",
    "#Test set\n",
    "pred_y_rf=rf.predict(X_credit_card_ss_test)\n",
    "\n",
    "print ('Recall score of RandomForest classifier on original set:{:.3f}'.format(recall_score(y_credit_card_ss_test,pred_y_rf)))\n",
    "print ('F1 score of RandomForest classifier on original set:{:.3f}'.format(f1_score(y_credit_card_ss_test,pred_y_rf)))\n",
    "print()\n",
    "cm_rf_original = confusion_matrix(y_credit_card_ss_test,pred_y_rf)\n",
    "print('Confusion matrix with RandomForest classifier on original set:\\n%s' % cm_rf_original)\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using isolation forest algorithm. It seems that isolated forest is suited for anomaly detection. \n",
    "#IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a \n",
    "#split value between the maximum and minimum values of the selected feature.Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n",
    "#This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n",
    "#Random partitioning produces noticeable shorter paths for anomalies. Hence, when a forest of random trees collectively\n",
    "#produce shorter path lengths for particular samples, they are highly likely to be anomalies.(https://scikit-learn.org/stable/auto_examples/ensemble/plot_isolation_forest.html)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "#Creating a training dataset containing only non fraud cases taken from the dataset \n",
    "X2_train = resample(credit_card_ss[credit_card_ss.Class==0], \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=199000,     # represent around ~70% of non-fraudulent cases\n",
    "                                 random_state=75) # reproducible results\n",
    "\n",
    "X2_isof = X2_train.drop('Class', axis = 1)\n",
    "\n",
    "#Creating a 2nd dataset containing left over data after 199000 cases of non fraud were sampled out. \n",
    "X_original_leftover = credit_card_ss.loc[~credit_card_ss.set_index(list(credit_card_ss.columns)).index.isin(X2_train.set_index(list(X2_train.columns)).index)]\n",
    "X_original_no_y = X_original_leftover.drop('Class', axis =1)\n",
    "y_from_X_original = X_original_leftover ['Class']\n",
    "#Training set \n",
    "isof = IsolationForest(behaviour='new',max_samples='auto',random_state=45)\n",
    "isof.fit(X2_isof)\n",
    "\n",
    "#Test set\n",
    "output_pred_y_score=pd.DataFrame((isof.predict(X_original_no_y)),columns =['isolation forest score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_original_leftover = X_original_leftover.reset_index() #Resetting index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_original_leftover.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_original_leftover=X_original_leftover.drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_leftover  = pd.concat([X_original_leftover, output_pred_y_score], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#original_leftover = X_original_leftover.join(output_pred_y_score).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.value_counts(original_leftover['isolation forest score'].values, sort=False) #checking the  count per socre (1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.value_counts(original_leftover['Class'].values, sort=False) #Checking the count per classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_leftover.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking the dataframe which has both class and isolation forest score \n",
    "original_leftover.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_leftover.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#To be able to get recall and F1 scores, I will have to do a count based on conditions: \n",
    "#Class =1 and Isolation forest score = 1 (Real Fraud but classified as non fraud, False negative)\n",
    "#Class = 1 and Isolation forest score =-1 (Real Fraud but classified as fraud, True positive)\n",
    "#Class = 0 and Isolation Forest score =1 (Real non fraud and classified as non fraud, True negative)\n",
    "#Class = 0 and Isolation Forest score =-1 (Read non fraud but classified as fraud, False positive )\n",
    "\n",
    "True_Negative = len(original_leftover[(original_leftover['Class']==0) & (original_leftover['isolation forest score']==1)])\n",
    "False_Positive = len(original_leftover[(original_leftover['Class']==0) & (original_leftover['isolation forest score']==-1.0)])\n",
    "True_Positive = len(original_leftover[(original_leftover['Class']==1) & (original_leftover['isolation forest score']==-1)])\n",
    "False_Negative = len(original_leftover[(original_leftover['Class']==1) & (original_leftover['isolation forest score']==1)])\n",
    "\n",
    "print('Number of fraud cases classified as non fraud with isolation forest is', \n",
    "      len(original_leftover[(original_leftover['Class']==1) & (original_leftover['isolation forest score']==1)]))\n",
    "print('Number of fraud cases classified as fraud with isolation forest is', \n",
    "      len(original_leftover[(original_leftover['Class']==1) & (original_leftover['isolation forest score']==-1)]))\n",
    "print('Number of non fraud cases classified as fraud with isolation forest is', \n",
    "      len(original_leftover[(original_leftover['Class']==0) & (original_leftover['isolation forest score']==-1)]))\n",
    "print('Number of non fraud cases classified as non fraud with isolation forest is', \n",
    "      len(original_leftover[(original_leftover['Class']==0) & (original_leftover['isolation forest score']==1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Recall_score = True_Positive/(True_Positive + False_Negative)\n",
    "Precision_score = True_Positive/(True_Positive + False_Positive)\n",
    "F1_score = 2*(Precision_score*Recall_score)/(Precision_score+Recall_score)\n",
    "print ('Recall score with isolation forest is {:.3f}'.format(Recall_score))\n",
    "print ('F1_score with isolation forest is {:.3f}'.format(F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Printing True_Negative, False_Positive, False_Negative, True_Positive as a confusion matrix for the isolation forest model\n",
    "print ('[[76452   8456]')\n",
    "print ( '[   49     443]]')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
